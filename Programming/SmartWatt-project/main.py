# -*- coding: utf-8 -*-
"""lo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SAKZJDxMLesulhkazqH-jn5P19fvnyfG
"""

import pandas as pd

#input CSV file
df=pd.read_csv('Dataset.csv')
df

df.describe()

#necessary for weather API
#!pip install meteostat

#added in weather data
from datetime import datetime
import matplotlib.pyplot as plt
from meteostat import Point, Hourly

# Define the time period and location
start = datetime(2022, 10, 4)
end = datetime(2024, 9, 30)
Houston = Point(29.7604, -95.3698, 13)

# Fetch hourly weather data
df1 = Hourly(Houston, start, end)
weather_df = df1.fetch()

# Convert temperature from Celsius to Fahrenheit
weather_df['temp_fahrenheit'] = weather_df['temp'] * 9 / 5 + 32

weather_df = weather_df.drop(columns=['dwpt','rhum','prcp','snow','wpgt','pres','tsun', 'wspd', 'wdir', 'coco','dwpt','temp'])
len(weather_df)
# Plot the Fahrenheit temperature
weather_df.plot(y=['temp_fahrenheit'], title='Hourly Temperature in Houston (Fahrenheit)')
plt.xlabel('Date and Time')
plt.ylabel('Temperature (°F)')
plt.show()

import matplotlib.pyplot as plt

# Create 'USAGE_START' as a combined datetime column before filtering for 'Alfa'
df['USAGE_START'] = pd.to_datetime(df['USAGE_DATE'].astype(str) + ' ' + df['USAGE_START_TIME'].astype(str))

alfa_df = df[df['ESIID'] == 'Alfa']
bravo_df = df[df['ESIID'] == 'Bravo']
charlie_df = df[df['ESIID'] == 'Charlie']
delta_df = df[df['ESIID'] == 'Delta']
echo_df = df[df['ESIID'] == 'Echo']
foxtrot_df = df[df['ESIID'] == 'Foxtrot']
golf_df = df[df['ESIID'] == 'Golf']
hotel_df = df[df['ESIID'] == 'Hotel']
india_df = df[df['ESIID'] == 'India']
juliet_df = df[df['ESIID'] == 'Juliet']
kilo_df = df[df['ESIID'] == 'Kilo']
lima_df = df[df['ESIID'] == 'Lima']



# Set 'USAGE_START' as the index for time-based resampling
alfa_df.set_index('USAGE_START', inplace=True)
bravo_df.set_index('USAGE_START', inplace=True)
charlie_df.set_index('USAGE_START', inplace=True)
delta_df.set_index('USAGE_START', inplace=True)
echo_df.set_index('USAGE_START', inplace=True)
foxtrot_df.set_index('USAGE_START', inplace=True)
golf_df.set_index('USAGE_START', inplace=True)
hotel_df.set_index('USAGE_START', inplace=True)
india_df.set_index('USAGE_START', inplace=True)
juliet_df.set_index('USAGE_START', inplace=True)
kilo_df.set_index('USAGE_START', inplace=True)
lima_df.set_index('USAGE_START', inplace=True)


# Resample to 1-hour intervals and sum 'USAGE_KWH' for each interval
one_hour_data = alfa_df['USAGE_KWH'].resample('1h').sum()


esiids = df['ESIID'].unique()



for esiid in esiids:
    # Filter data for the current ESIID
    esiid_df = df[df['ESIID'] == esiid]

    # Set 'USAGE_START' as the index for time-based resampling
    esiid_df.set_index('USAGE_START', inplace=True)

    # Resample to 6-hour intervals and sum 'USAGE_KWH' for each interval
    one_hour_data = esiid_df['USAGE_KWH'].resample('1h').sum()

    # Plot the 6-hour aggregated data for the current ESIID
    plt.figure(figsize=(30, 6))
    plt.plot(one_hour_data.index, one_hour_data, label=f'1-Hour Aggregated Energy Usage (KWH) for {esiid}', color='purple')
    plt.title(f'1-Hour Aggregated Energy Usage for {esiid} Over Time')
    plt.xlabel('Time')
    plt.ylabel('Usage (KWH)')
    plt.legend()


    # Display the resampled DataFrame for the current ESIID
    print(f"1-Hour Aggregated DataFrame for {esiid}", one_hour_data)

len(one_hour_data)

# Set 'USAGE_START' as the index in alfa_df if not already
alfa_df = alfa_df.reset_index()

# Ensure both dataframes have datetime-compatible keys
alfa_df['USAGE_START'] = pd.to_datetime(alfa_df['USAGE_START'])
weather_df = weather_df.reset_index()
weather_df['time'] = pd.to_datetime(weather_df['time'])

# Sort both dataframes by their datetime columns
alfa_df = alfa_df.sort_values('USAGE_START')
weather_df = weather_df.sort_values('time')
print("MERGED DATA")
# Use merge_asof to align weather data to usage intervals
merged_data = pd.merge_asof(
    alfa_df,
    weather_df[['time', 'temp_fahrenheit']],  # Keep only time and temperature
    left_on='USAGE_START',  # Usage timestamp
    right_on='time',        # Closest hourly timestamp
    direction='backward'    # Match the closest previous hourly timestamp
)

# Drop unnecessary columns if needed
merged_data = merged_data.drop(columns=['time'])

# Print the resulting dataset
print(merged_data)

print(len(alfa_df), len(weather_df), len(merged_data))

import matplotlib.pyplot as plt

# Ensure merged_data contains both 'USAGE_KWH' and 'temp_fahrenheit'
plt.figure(figsize=(10, 6))
plt.scatter(merged_data['temp_fahrenheit'], merged_data['USAGE_KWH'], alpha=0.5, color='blue', edgecolor='k')

# Add titles and labels
plt.title('Energy Usage vs Temperature', fontsize=16)
plt.xlabel('Temperature (°F)', fontsize=14)
plt.ylabel('Energy Usage (KWH)', fontsize=14)
plt.grid(True)
plt.show()

import math
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Input, LSTM, GRU, Dropout
from keras.optimizers import SGD
from sklearn.preprocessing import MinMaxScaler

def create_sequences(data, sequence_length):
    X, y = [], []
    for i in range(sequence_length, len(data)):
        X.append(data[i-sequence_length:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

#splits into 80% training %20 testing
'''
training_alfa_len = math.ceil(len(alfa_df)*.8)
training_alfa_len

train_data = alfa_df[:training_alfa_len]
test_data = alfa_df[training_alfa_len:]

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(alfa_df['USAGE_KWH'].values.reshape(-1, 1))
'''

#uses alfa dataset did not want to change var names for foxtrot :)
training_foxtrot_len = math.ceil(len(merged_data)*.8)
training_foxtrot_len

train_data = merged_data[:training_foxtrot_len]
test_data = merged_data[training_foxtrot_len:]

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(merged_data['USAGE_KWH'].values.reshape(-1, 1))

train_scaled = scaled_data[:training_foxtrot_len] # Assign the scaled training data to train_scaled
test_scaled = scaled_data[training_foxtrot_len:]  # Assign the scaled testing data to test_scaled


sequence_length = 60
X_train, y_train = create_sequences(train_scaled, sequence_length)
X_test, y_test = create_sequences(test_scaled, sequence_length)

simple_RNN = Sequential([
    Input(shape = (sequence_length, 1)),
    SimpleRNN(50, activation = 'relu', return_sequences = True),
    SimpleRNN(50, activation = 'relu', return_sequences = False),
    Dense(25, activation = 'relu'),
    Dense(1)
])
simple_RNN.summary()
simple_RNN.compile(optimizer = 'adam', loss = 'mse')
simple_RNN.fit(X_train, y_train, epochs = 3, batch_size = 128, validation_data = (X_test, y_test))
loss = simple_RNN.evaluate(X_test, y_test)



print(f"Test Loss: {loss}")
predictions = simple_RNN.predict(X_test)

# Reshape data to 3D shape for LSTM
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

regressorLSTM = Sequential()
regressorLSTM.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
regressorLSTM.add(LSTM(units=50, return_sequences=False))
regressorLSTM.add(Dense(units=25))
regressorLSTM.add(Dense(units=1))
regressorLSTM.summary()

regressorLSTM.compile(optimizer='adam', loss='mean_squared_error')
regressorLSTM.fit(X_train, y_train, epochs=3, batch_size=128, validation_data = (X_test, y_test))

predicted_data = regressorLSTM.predict(X_test)
predicted_data_unscaled1 = scaler.inverse_transform(predicted_data)
y_test_unscaled1 = scaler.inverse_transform(y_test.reshape(-1, 1))

plt.figure(figsize=(30, 6))
plt.plot(y_test_unscaled1, label='Actual Energy Usage (KWH)', color='blue')
plt.title('ActualEnergy Usage')
plt.xlabel('Time')
plt.ylabel('Energy Usage (KWH)')
plt.legend()
plt.show()

plt.figure(figsize=(30, 6))
plt.plot(predicted_data_unscaled1, label='Predicted Energy Usage (KWH)', color='orange')
plt.title('Predicted Energy Usage')
plt.xlabel('Time')
plt.ylabel('Energy Usage (KWH)')
plt.legend()
plt.show()

# GRU Model
regressorGRU = Sequential()

# GRU layers with Dropout regularisation
regressorGRU.add(GRU(units=50,
                     return_sequences=True,
                     input_shape=(X_train.shape[1],1),
                     activation='tanh'))
regressorGRU.add(Dropout(0.2))

regressorGRU.add(GRU(units=50,
                     return_sequences=False,
                     activation='tanh'))

# The output layer
regressorGRU.add(Dense(units=1,
                       activation='relu'))
# Compiling the RNN
regressorGRU.compile(optimizer=SGD(learning_rate=0.01,
                                   decay=1e-7,
                                   momentum=0.9,
                                   nesterov=False),
                     loss='mean_squared_error')

# Fitting the data
regressorGRU.fit(X_train, y_train, epochs=3, batch_size=128,validation_data = (X_test, y_test))
regressorGRU.summary()

prediction_GRU = regressorGRU.predict(X_test)
predicted_data_unscaled2 = scaler.inverse_transform(prediction_GRU)
y_test_unscaled2 = scaler.inverse_transform(y_test.reshape(-1, 1))

plt.figure(figsize=(30, 6))
plt.plot(y_test_unscaled2, label='Actual Energy Usage (KWH)', color='blue')
plt.title('ActualEnergy Usage')
plt.xlabel('Time')
plt.ylabel('Energy Usage (KWH)')
plt.legend()
plt.show()

plt.figure(figsize=(30, 6))
plt.plot(predicted_data_unscaled2, label='Predicted Energy Usage (KWH)', color='orange')
plt.title('Predicted Energy Usage')
plt.xlabel('Time')
plt.ylabel('Energy Usage (KWH)')
plt.legend()
plt.show()

predicted_data_unscaled = scaler.inverse_transform(predictions)
y_test_unscaled = scaler.inverse_transform(y_test.reshape(-1, 1))

plt.figure(figsize=(30, 6))
plt.plot(y_test_unscaled, label='Actual Energy Usage (KWH)', color='blue')
plt.title('ActualEnergy Usage using Simple ')
plt.xlabel('Time')
plt.ylabel('Energy Usage (KWH)')
plt.legend()
plt.show()

plt.figure(figsize=(30, 6))
plt.plot(predicted_data_unscaled, label='Predicted Energy Usage (KWH)', color='orange')
plt.title('Predicted Energy Usage')
plt.xlabel('Time')
plt.ylabel('Energy Usage (KWH)')
plt.legend()
plt.show()

# Plot the training, test, and predicted data together
plt.figure(figsize=(45, 12))

# Training data
plt.plot(train_data.index, train_data['USAGE_KWH'], label='Training Data', color='blue')

# Test data
plt.plot(test_data.index, test_data['USAGE_KWH'], label='Test Data', color='green')

# Predicted data
test_time_index = test_data.index[sequence_length:len(predicted_data_unscaled) + sequence_length]
plt.plot(test_time_index, predicted_data_unscaled, label='Predicted Data (LSTM)', color='orange')

# Adding title and labels
plt.title('Energy Usage Prediction (Train vs Test vs Predicted)', fontsize=18)
plt.xlabel('Time', fontsize=14)
plt.ylabel('Energy Usage (KWH)', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()

# GRU Plot

# Plot the training, test, and predicted data together
plt.figure(figsize=(45, 12))

# Training data
plt.plot(train_data.index, train_data['USAGE_KWH'], label='Training Data', color='blue')

# Test data
plt.plot(test_data.index, test_data['USAGE_KWH'], label='Test Data', color='green')

# Predicted data
test_time_index = test_data.index[sequence_length:len(predicted_data_unscaled) + sequence_length]
plt.plot(test_time_index, predicted_data_unscaled, label='Predicted Data (GRU)', color='orange')

# Adding title and labels
plt.title('Energy Usage Prediction (Train vs Test vs Predicted)', fontsize=18)
plt.xlabel('Time', fontsize=14)
plt.ylabel('Energy Usage (KWH)', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()

#We want to predict based on the trends we created in our training the model.
#start the FORECASTING in about 1-2 months.
#How do we do that?

last_sequence = scaled_data[-sequence_length:]
input_sequence = np.reshape(last_sequence, (1, sequence_length, 1))

forecast_steps = 30 * 4

forecasted_data = []

for _ in range(forecast_steps):
    prediction = regressorGRU.predict(input_sequence)[0, 0]
    prediction += np.random.normal(0, 0.06)
    forecasted_data.append(prediction)

    input_sequence = np.append(input_sequence[:, 1:, :], [[[prediction]]], axis=1)

future_predictions_unscaled = scaler.inverse_transform(np.array(forecasted_data).reshape(-1, 1))

last_timestamp = foxtrot_df.index[-1]
future_timestamps = pd.date_range(start=last_timestamp, periods=forecast_steps, freq='6h')

forecast_df = pd.DataFrame(data=future_predictions_unscaled, index=future_timestamps, columns=['Forecasted Usage'])

# Plot historical data
plt.figure(figsize=(55, 10))
plt.plot(foxtrot_df.index, foxtrot_df['USAGE_KWH'], label='Historical Data', color='blue')

# Plot forecasted data
plt.plot(forecast_df.index, forecast_df['Forecasted Usage'], label='Forecasted Data', color='red')
upper_bound = future_predictions_unscaled + (1.96 * 0.05)  # 95% confidence interval
lower_bound = future_predictions_unscaled - (1.96 * 0.05)

plt.fill_between(future_timestamps, lower_bound.flatten(), upper_bound.flatten(),
                 color='yellow', alpha=0.3, label='95% Confidence Interval')
# Add title and labels
plt.title('Energy Usage Forecast (Extended)', fontsize=18)
plt.xlabel('Time', fontsize=14)
plt.ylabel('Energy Usage (KWH)', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()